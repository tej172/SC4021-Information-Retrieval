{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yonghui/opt/anaconda3/envs/py310/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from textblob import TextBlob\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import re\n",
    "\n",
    "\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-irony\"\n",
    "sarcasm_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "sarcasm_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    text = re.sub(r\"http\\S+\", \"http\", text)  \n",
    "    text = re.sub(r\"@\\w+\", \"@user\", text)    \n",
    "    return text\n",
    "\n",
    "def detect_sarcasm(text: str) -> bool:\n",
    "    \"\"\"Returns True if text is sarcastic, False otherwise.\"\"\"\n",
    "    text = preprocess_text(text)  \n",
    "\n",
    "\n",
    "    inputs = sarcasm_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = sarcasm_model(**inputs).logits\n",
    "    probs = torch.softmax(logits, dim=1).squeeze()\n",
    "\n",
    "    \n",
    "    return probs[1].item() > 0.5  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_only(text):\n",
    "    analysis = TextBlob(text)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return \"Positive\"\n",
    "    elif analysis.sentiment.polarity < 0:\n",
    "        return \"Negative\"\n",
    "    return \"Neutral\"\n",
    "\n",
    "def polarity_then_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    polarity = analysis.sentiment.polarity\n",
    "    subjectivity = analysis.sentiment.subjectivity\n",
    "\n",
    "    \n",
    "    if polarity > 0:\n",
    "        sentiment_direction = \"Positive\"\n",
    "    elif polarity < 0:\n",
    "        sentiment_direction = \"Negative\"\n",
    "    else:\n",
    "        sentiment_direction = \"Neutral\"\n",
    "\n",
    "    \n",
    "    if sentiment_direction == \"Positive\":\n",
    "        pos_threshold = 0.1 if subjectivity > 0.5 else 0.2\n",
    "        return \"Positive\" if polarity > pos_threshold else \"Neutral\"\n",
    "    \n",
    "    elif sentiment_direction == \"Negative\":\n",
    "        neg_threshold = -0.1 if subjectivity > 0.5 else -0.2\n",
    "        return \"Negative\" if polarity < neg_threshold else \"Neutral\"\n",
    "    \n",
    "    else:\n",
    "        return \"Neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_sentiment(text: str) -> str:\n",
    "  \n",
    "    polarity = TextBlob(text).sentiment.polarity\n",
    "    is_sarcastic = detect_sarcasm(text)\n",
    "\n",
    "   \n",
    "    if is_sarcastic:\n",
    "        polarity = -polarity  \n",
    "\n",
    "    if polarity > 0.2:\n",
    "        return \"Positive\"\n",
    "    elif polarity < -0.2:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sentiment Only ===\n",
      "Accuracy:  0.3333\n",
      "Precision: 0.4000\n",
      "Recall:    0.5000\n",
      "F1 Score:  0.3333\n",
      "\n",
      "=== Polarity then Sentiment ===\n",
      "Accuracy:  0.3333\n",
      "Precision: 0.2500\n",
      "Recall:    0.5000\n",
      "F1 Score:  0.3000\n",
      "\n",
      "=== Enhanced Sentiment (with Sarcasm) ===\n",
      "Accuracy:  0.8333\n",
      "Precision: 0.8889\n",
      "Recall:    0.8889\n",
      "F1 Score:  0.8667\n"
     ]
    }
   ],
   "source": [
    "def evaluate_methods(dataset):\n",
    "    methods = {\n",
    "        \"Sentiment Only\": sentiment_only,\n",
    "        \"Polarity then Sentiment\": polarity_then_sentiment,\n",
    "        \"Enhanced Sentiment (with Sarcasm)\": enhanced_sentiment\n",
    "    }\n",
    "\n",
    "    results = {\n",
    "        method: {\n",
    "            \"y_true\": [],\n",
    "            \"y_pred\": []\n",
    "        }\n",
    "        for method in methods\n",
    "    }\n",
    "\n",
    "    for entry in dataset:\n",
    "        text = entry[\"text\"]\n",
    "        \n",
    "        ann1 = entry[\"manual_labels\"][\"annotator1\"][\"polarity\"]\n",
    "        ann2 = entry[\"manual_labels\"][\"annotator2\"][\"polarity\"]\n",
    "        ground_truth = ann1 if ann1 == ann2 else \"Neutral\"\n",
    "\n",
    "        for method_name, method_func in methods.items():\n",
    "            pred = method_func(text)\n",
    "            results[method_name][\"y_true\"].append(ground_truth)\n",
    "            results[method_name][\"y_pred\"].append(pred)\n",
    "\n",
    "    for method_name in methods:\n",
    "        y_true = results[method_name][\"y_true\"]\n",
    "        y_pred = results[method_name][\"y_pred\"]\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average=\"macro\", zero_division=0\n",
    "        )\n",
    "\n",
    "        print(f\"\\n=== {method_name} ===\")\n",
    "        print(f\"Accuracy:  {acc:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall:    {recall:.4f}\")\n",
    "        print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n",
    "with open(\"evaluationDataset.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "evaluate_methods(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
