{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewhRl5yXQWuh"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnxOe7orPoIw"
      },
      "outputs": [],
      "source": [
        "##############################################\n",
        "# 1) SARCASM DETECTION\n",
        "##############################################\n",
        "# Use a pre-trained Hugging Face model specialized in detecting irony/sarcasm.\n",
        "#   Model card: https://huggingface.co/cardiffnlp/twitter-roberta-base-irony\n",
        "# Install: pip install transformers torch\n",
        "# This model is trained on Twitter data\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "SARCASTIC_LABEL = \"sarcastic\"\n",
        "NOT_SARCASTIC_LABEL = \"not_sarcastic\"\n",
        "\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "sarcasm_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "sarcasm_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "def detect_sarcasm(text: str) -> str:\n",
        "    inputs = sarcasm_tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
        "    with torch.no_grad():\n",
        "        logits = sarcasm_model(**inputs).logits\n",
        "    probs = torch.softmax(logits, dim=1).squeeze()\n",
        "    # Label 0 = non-irony, Label 1 = irony\n",
        "    if probs[1].item() > 0.5:\n",
        "        return SARCASTIC_LABEL\n",
        "    else:\n",
        "        return NOT_SARCASTIC_LABEL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFliM_lZdeMP"
      },
      "outputs": [],
      "source": [
        "# !python -m spacy download en_core_web_trf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbLfe3dqPqOH",
        "outputId": "b970d0ab-c8bb-47f0-e72a-ead78196780e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using en_core_web_trf for advanced NER.\n"
          ]
        }
      ],
      "source": [
        "##############################################\n",
        "# 2) NAMED ENTITY RECOGNITION (NER)\n",
        "##############################################\n",
        "# We'll use spaCy's English model\n",
        "# Install: pip install spacy\n",
        "# Then: python -m spacy download en_core_web_sm\n",
        "# Then: python -m spacy download en_core_web_trf\n",
        "\n",
        "import spacy\n",
        "\n",
        "# 1) Load an advanced spaCy model (fallback to en_core_web_sm if needed)\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_trf\")\n",
        "    print(\"Using en_core_web_trf for advanced NER.\")\n",
        "except:\n",
        "\n",
        "    # dont fall aback to smallermodel for better results\n",
        "    # nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"Falling back to en_core_web_sm.\")\n",
        "\n",
        "\n",
        "def do_ner(text: str):\n",
        "    \"\"\"\n",
        "    Return a list of recognized named entities (text + label).\n",
        "    Example output: [{\"entity\": \"AI\", \"label\": \"ORG\"}, ...]\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    entities = []\n",
        "    for ent in doc.ents:\n",
        "        entities.append({\"entity\": ent.text, \"label\": ent.label_})\n",
        "\n",
        "    return entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEdgCRS0efNd",
        "outputId": "27588087-7c21-4b04-9fd9-fbc9d4f4ab65"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTvMekpJQeKj"
      },
      "outputs": [],
      "source": [
        "##############################################\n",
        "# 3) CONCEPT EXTRACTION\n",
        "##############################################\n",
        "# Concept extraction can be domain-specific. \n",
        "# that looks for keywords/phrases in the text and groups them under certain \"concepts\".\n",
        "# In real practice, you might use a more sophisticated approach (e.g., ontology mapping).\n",
        "\n",
        "\n",
        "concept_dictionary = {\n",
        "    \"jobs_and_careers\": [\n",
        "        \"job\", \"jobs\", \"career\", \"hire\", \"hiring\", \"position\", \"occupation\", \"employment\",\n",
        "        \"recruitment\", \"headhunting\", \"resum√©\", \"job search\", \"job application\", \"interview\",\n",
        "        \"promotion\", \"layoff\", \"downsizing\", \"resignation\", \"fired\", \"retrenched\", \"workforce\",\n",
        "        \"unemployment\", \"job board\", \"career change\", \"gig economy\", \"contract work\",\n",
        "        \"freelance\", \"upskilling\", \"reskilling\", \"talent shortage\", \"skills gap\",\n",
        "        \"career transition\", \"remote job\", \"flexible work\", \"hybrid work\"\n",
        "    ],\n",
        "\n",
        "    \"ai_tech\": [\n",
        "        \"ai\", \"artificial intelligence\", \"ml\", \"machine learning\", \"deep learning\",\n",
        "        \"neural network\", \"natural language processing\", \"NLP\", \"computer vision\",\n",
        "        \"generative ai\", \"chatgpt\", \"gpt-4\", \"llm\", \"transformer model\", \"reinforcement learning\",\n",
        "        \"automation\", \"autonomous system\", \"robot\", \"ai-powered\", \"ai-driven\",\n",
        "        \"ai algorithm\", \"ai model\", \"ai tool\", \"ai system\", \"ai platform\", \"ai assistant\",\n",
        "        \"predictive modeling\", \"intelligent agent\", \"intelligent automation\", \"digital worker\",\n",
        "        \"AI ethics\", \"AI governance\", \"AI safety\", \"AI alignment\", \"AI regulation\",\n",
        "        \"OpenAI\", \"Hugging Face\", \"LangChain\", \"Chatbot\", \"BERT\", \"GPT\", \"GAN\",\n",
        "        \"AI job disruption\", \"AI replacing jobs\", \"AI impact\", \"AI revolution\"\n",
        "    ],\n",
        "\n",
        "    \"job_market_trends\": [\n",
        "        \"job market\", \"labor market\", \"workforce trends\", \"employment trends\",\n",
        "        \"market shift\", \"skills demand\", \"future of work\", \"career outlook\",\n",
        "        \"talent acquisition\", \"workforce transformation\", \"job automation\",\n",
        "        \"digital transformation\", \"economic uncertainty\", \"AI economy\", \"skills of the future\",\n",
        "        \"AI in hiring\", \"job displacement\", \"employment shift\", \"workplace change\"\n",
        "    ],\n",
        "\n",
        "    \"finance\": [\n",
        "        \"bank\", \"investment\", \"finance\", \"trading\", \"fintech\", \"income\", \"earnings\",\n",
        "        \"financial security\", \"salary\", \"paycheck\", \"minimum wage\", \"wealth gap\",\n",
        "        \"cost of living\", \"job loss compensation\", \"recession\", \"economic downturn\",\n",
        "        \"furlough\", \"financial hardship\", \"severance\", \"retirement fund\"\n",
        "    ],\n",
        "\n",
        "    \"mental_health\": [\n",
        "        \"stress\", \"burnout\", \"anxiety\", \"job stress\", \"mental health\", \"wellbeing\",\n",
        "        \"work-life balance\", \"job insecurity\", \"career anxiety\", \"AI anxiety\"\n",
        "    ],\n",
        "\n",
        "    \"education_training\": [\n",
        "        \"upskilling\", \"reskilling\", \"online course\", \"certification\", \"MOOC\", \"bootcamp\",\n",
        "        \"lifelong learning\", \"career coaching\", \"learning path\", \"digital skills\",\n",
        "        \"AI literacy\", \"tech training\", \"coursework\", \"workforce development\"\n",
        "    ],\n",
        "\n",
        "     \"automation_and_displacement\": [\n",
        "        \"automation\", \"job automation\", \"automated process\", \"workflow automation\",\n",
        "        \"displaced workers\", \"displacement\", \"automated job loss\", \"automated replacement\",\n",
        "        \"robotic process automation\", \"RPA\", \"bots replacing humans\", \"AI takeover\",\n",
        "        \"human redundancy\", \"task automation\"\n",
        "    ],\n",
        "\n",
        "    \"policy_and_governance\": [\n",
        "        \"universal basic income\", \"UBI\", \"AI regulation\", \"labor law\", \"AI tax\",\n",
        "        \"tech policy\", \"future legislation\", \"AI governance\", \"worker protection\",\n",
        "        \"job guarantee\", \"government retraining\", \"economic policy\", \"union response\",\n",
        "        \"digital rights\", \"job security policy\"\n",
        "    ],\n",
        "\n",
        "    \"recruitment_technology\": [\n",
        "        \"AI recruitment\", \"AI hiring\", \"talent filter\", \"algorithmic hiring\",\n",
        "        \"resume screening\", \"candidate ranking\", \"job matching platform\", \"talent intelligence\",\n",
        "        \"hiring automation\", \"virtual interview\", \"digital HR\", \"ATS\", \"HR tech\"\n",
        "    ],\n",
        "\n",
        "    \"remote_and_gig_work\": [\n",
        "        \"gig economy\", \"freelancing\", \"remote job\", \"remote-first\", \"digital nomad\",\n",
        "        \"platform work\", \"side hustle\", \"Uberization\", \"creator economy\", \"independent worker\",\n",
        "        \"contractor\", \"online work\", \"flex work\", \"microtask\", \"Upwork\", \"Fiverr\"\n",
        "    ],\n",
        "\n",
        "    \"public_sentiment_discourse\": [\n",
        "        \"boomer\", \"doomer\", \"tech bro\", \"jobpocalypse\", \"decel\", \"quiet quitting\",\n",
        "        \"great resignation\", \"job hopping\", \"layoff wave\", \"hustle culture\", \"AI hype\",\n",
        "        \"future fear\", \"doomscrolling\", \"LinkedIn post\", \"upskilling frenzy\", \"prompt engineering\"\n",
        "    ],\n",
        "\n",
        "    \"tech_company_trends\": [\n",
        "        \"OpenAI\", \"Google DeepMind\", \"Meta AI\", \"Anthropic\", \"Stability AI\", \"Amazon layoffs\",\n",
        "        \"tech layoffs\", \"hiring freeze\", \"big tech\", \"FAANG\", \"tech exodus\", \"startup layoffs\",\n",
        "        \"VC funding freeze\", \"AI startup\", \"early retirement\", \"LinkedIn hiring trends\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyS0IZjWe7S2"
      },
      "outputs": [],
      "source": [
        "# ! pip install rapidfuzz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7t1QFSFfA_F"
      },
      "outputs": [],
      "source": [
        "iimport re\n",
        "from spacy.matcher import PhraseMatcher\n",
        "# For fuzzy matching (preferred over fuzzywuzzy)\n",
        "# pip install rapidfuzz\n",
        "from rapidfuzz import fuzz\n",
        "from rapidfuzz import process as fuzz_process\n",
        "\n",
        "# Preprocess PhraseMatcher for multi-word terms\n",
        "phrase_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
        "for cat, keywords in concept_dictionary.items():\n",
        "    patterns = [nlp.make_doc(kw) for kw in keywords if len(kw.split()) > 1]\n",
        "    if patterns:\n",
        "        phrase_matcher.add(cat, patterns)\n",
        "\n",
        "def lemmatized_tokens(doc):\n",
        "    return [token.lemma_.lower() for token in doc if token.is_alpha]\n",
        "\n",
        "def do_concept_extraction_v3(text: str, fuzzy_threshold=88):\n",
        "    doc = nlp(text)\n",
        "    matched_concepts = set()\n",
        "\n",
        "    # Phrase matching (multi-word only)\n",
        "    matches = phrase_matcher(doc)\n",
        "    for match_id, start, end in matches:\n",
        "        matched_concepts.add(nlp.vocab.strings[match_id])\n",
        "\n",
        "    # Token lemmatization\n",
        "    tokens = lemmatized_tokens(doc)\n",
        "\n",
        "    # Fuzzy matching for single-word concepts only\n",
        "    for cat, keywords in concept_dictionary.items():\n",
        "        for kw in keywords:\n",
        "            if len(kw.split()) == 1:\n",
        "                for token in tokens:\n",
        "                    score = fuzz.partial_ratio(token, kw.lower())\n",
        "                    if score >= fuzzy_threshold:\n",
        "                        matched_concepts.add(cat)\n",
        "                        break  # Avoid multiple adds per category\n",
        "\n",
        "    return list(matched_concepts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHgJ77fye-8x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_rh01ejQg5l"
      },
      "outputs": [],
      "source": [
        "##############################################\n",
        "# 4) ASPECT EXTRACTION\n",
        "##############################################\n",
        "# Aspect extraction typically appears in \"Aspect-Based Sentiment Analysis (ABSA).\"\n",
        "# The code below is a *placeholder* showing how you might:\n",
        "#   1. Identify candidate aspect terms (often nouns or noun phrases)\n",
        "#   2. Return them as ‚Äúaspects‚Äù for each post\n",
        "# For a more advanced approach, check out open-source ABSA libraries or spacy patterns.\n",
        "\n",
        "def do_aspect_extraction(text: str):\n",
        "    \"\"\"\n",
        "    Simple approach: collect noun chunks as 'aspects'.\n",
        "    Example: \"AI is transforming the job market.\" -> [\"AI\", \"the job market\"]\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    aspects = []\n",
        "    for chunk in doc.noun_chunks:\n",
        "        aspects.append(chunk.text.strip())\n",
        "    return aspects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hn3iED2DfkNK"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Richer Aspect Extraction using/;\n",
        "Instead of returning every noun_chunk, we can use dependency patterns to identify\n",
        "aspects that have ‚Äúdescriptive context‚Äù (e.g., adjectives or prepositional modifiers).\n",
        "\n",
        "We can attempt to collect the head noun plus its modifiers (adjectival, compound)\n",
        "to form a more coherent aspect phrase.\n",
        "'''\n",
        "def do_aspect_extraction_v2(text: str):\n",
        "    \"\"\"\n",
        "    Extract aspects by looking for 'noun phrases' that include\n",
        "    any of the following:\n",
        "      - Adjectival modifiers (amod)\n",
        "      - Compound nouns\n",
        "      - Prepositional modifiers (nmod) linking nouns\n",
        "    This approach can yield more descriptive aspects:\n",
        "      e.g., \"powerful AI model\", \"advanced deep learning techniques\"\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    aspects = []\n",
        "\n",
        "    # We'll store aspect phrases by their head noun token\n",
        "    visited_heads = set()\n",
        "\n",
        "    for token in doc:\n",
        "        # If it's a noun that isn't a pronoun or 'dummy' token\n",
        "        if token.pos_ == \"NOUN\" and token.lemma_ not in [\"thing\", \"something\", \"someone\"]:\n",
        "            if token.dep_ in [\"nsubj\", \"dobj\", \"pobj\", \"ROOT\", \"attr\", \"conj\"]:\n",
        "                aspect_span = _expand_aspect(token)\n",
        "                # De-duplicate\n",
        "                if aspect_span not in visited_heads:\n",
        "                    visited_heads.add(aspect_span)\n",
        "                    aspects.append(aspect_span)\n",
        "\n",
        "    return aspects\n",
        "\n",
        "def _expand_aspect(head_token):\n",
        "    \"\"\"\n",
        "    Expand a noun token to include its adjectival or compound modifiers.\n",
        "    We'll gather left-side adjectives, compound nouns, etc.\n",
        "    Example:\n",
        "      Head: \"model\"\n",
        "      Left modifies: \"powerful\", \"AI\"\n",
        "      => \"powerful AI model\"\n",
        "    \"\"\"\n",
        "    # Gather all tokens that are part of this aspect\n",
        "    min_i = head_token.i\n",
        "    max_i = head_token.i\n",
        "    for child in head_token.children:\n",
        "        # If there's a prepositional structure \"model of something\", you can add logic here\n",
        "        pass\n",
        "\n",
        "    # Check left siblings for compounds or amod\n",
        "    for left in reversed(list(head_token.lefts)):\n",
        "        if left.dep_ in [\"amod\", \"compound\", \"nmod\", \"nn\"] or left.pos_ in [\"ADJ\", \"NOUN\"]:\n",
        "            min_i = min(min_i, left.i)\n",
        "        else:\n",
        "            # If we hit a token that isn't describing this noun, break out\n",
        "            break\n",
        "\n",
        "    # Build the text from min_i to max_i\n",
        "    aspect_span = head_token.doc[min_i : max_i + 1].text\n",
        "    return aspect_span"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTQUGAZhQi8O",
        "outputId": "1f4114cd-f43a-4b11-8a82-70a04b558ba2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing record 0/4\n",
            "All done! Check your output file for updated JSON data.\n",
            "Output saved to /content/drive/MyDrive/x_data/testing_x_output.json\n",
            "Original data count: 4\n"
          ]
        }
      ],
      "source": [
        "##############################################\n",
        "# PROCESS JSON DATA\n",
        "##############################################\n",
        "if __name__ == \"__main__\":\n",
        "    # REPLACE WITH YOUR ACTUAL FILENAME\n",
        "    filename = r\"/content/drive/MyDrive/x_data/testing_x_data_file.json\"\n",
        "    output_filename = r\"/content/drive/MyDrive/x_data/testing_x_output.json\"\n",
        "\n",
        "    # Load JSON\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as infile:\n",
        "        data = json.load(infile)\n",
        "\n",
        "    # Process each record\n",
        "    for i, record in enumerate(data):\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Processing record {i}/{len(data)}\")\n",
        "\n",
        "        text = record.get(\"text\", \"\")\n",
        "\n",
        "        # 1) Sarcasm detection\n",
        "        record[\"sarcasm_label\"] = detect_sarcasm(text)  # \"sarcastic\" or \"not_sarcastic\"\n",
        "\n",
        "        # 2) Named Entity Recognition\n",
        "        record[\"named_entities\"] = do_ner(text)\n",
        "\n",
        "        # 3) Concept Extraction\n",
        "        record[\"concepts_v3\"] = do_concept_extraction_v3(text)\n",
        "\n",
        "        # 4) Aspect Extraction\n",
        "        record[\"aspects\"] = do_aspect_extraction(text)\n",
        "        record[\"aspects_v2\"] = do_aspect_extraction_v2(text)\n",
        "\n",
        "\n",
        "    # Save updated JSON\n",
        "    with open(output_filename, \"w\", encoding=\"utf-8\") as outfile:\n",
        "        json.dump(data, outfile, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(\"All done! Check your output file for updated JSON data.\")\n",
        "    print(f\"Output saved to {output_filename}\")\n",
        "    print(f\"Original data count: {len(data)}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zEV70-pjMQL",
        "outputId": "7b9f1cd1-93f5-4e20-8d64-ad62d718ea8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "Record #1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "üìå Text:\n",
            "\n",
            "('This past year, many folks left their jobs to find better, more worthwhile '\n",
            " 'opportunities. Director of Sales Max Johns shares 3 tips for anyone hoping '\n",
            " 'to migrate into the fastest growing field; #machinelearning. #ai '\n",
            " '#datacentric #jobs #gethired https://t.co/ZEZxyPtboZ')\n",
            "==========\n",
            "üåÄ Sarcasm Detection: not_sarcastic\n",
            "==========\n",
            "\n",
            "üîç Named  (spaCy):\n",
            "  - This past year (DATE)\n",
            "  - Max Johns (PERSON)\n",
            "  - 3 (CARDINAL)\n",
            "\n",
            "üîç Named Entities_V2_(HuggingFace):\n",
            "  - Max Johns [PER] (score=1.00)\n",
            "==========\n",
            "\n",
            "üß† Concepts:\n",
            "  - jobs_and_careers\n",
            "  - retail_ecommerce\n",
            "  - transportation_logistics\n",
            "\n",
            "üß† Concepts_V2_:\n",
            "==========\n",
            "\n",
            "üîß Aspects:\n",
            "  - many folks\n",
            "  - their jobs\n",
            "  - better, more worthwhile opportunities\n",
            "  - Sales\n",
            "  - 3 tips\n",
            "  - anyone\n",
            "  - the fastest growing field\n",
            "  - machinelearning\n",
            "\n",
            "üîß Aspects_V2_:\n",
            "  - many folks\n",
            "  - jobs\n",
            "  - worthwhile opportunities\n",
            "  - tips\n",
            "  - growing field\n",
            "====================================================================================================\n",
            "Record #2\n",
            "----------------------------------------------------------------------------------------------------\n",
            "üìå Text:\n",
            "\n",
            "('Will technologies like #AI continue to change the face of business and #jobs '\n",
            " 'in the new year? Yes, according to this @ToolboxforB2B article. What do you '\n",
            " 'think the biggest #worktrend will be in #2022?     https://t.co/YHMujdIq1n')\n",
            "==========\n",
            "üåÄ Sarcasm Detection: not_sarcastic\n",
            "==========\n",
            "\n",
            "üîç Named  (spaCy):\n",
            "  - the new year (DATE)\n",
            "  - #2022 (DATE)\n",
            "\n",
            "üîç Named Entities_V2_(HuggingFace):\n",
            "  - AI [MISC] (score=1.00)\n",
            "  - Too [ORG] (score=0.83)\n",
            "  - ##box [ORG] (score=0.89)\n",
            "==========\n",
            "\n",
            "üß† Concepts:\n",
            "  - jobs_and_careers\n",
            "\n",
            "üß† Concepts_V2_:\n",
            "==========\n",
            "\n",
            "üîß Aspects:\n",
            "  - technologies\n",
            "  - AI\n",
            "  - the face\n",
            "  - business\n",
            "  - #\n",
            "  - jobs\n",
            "  - the new year\n",
            "  - this @ToolboxforB2B article\n",
            "  - you\n",
            "  - the biggest #\n",
            "  - worktrend\n",
            "\n",
            "üîß Aspects_V2_:\n",
            "  - technologies\n",
            "  - face\n",
            "  - business\n",
            "  - #\n",
            "  - jobs\n",
            "  - new year\n",
            "  - @ToolboxforB2B article\n",
            "  - biggest #\n",
            "  - worktrend\n",
            "====================================================================================================\n",
            "Record #3\n",
            "----------------------------------------------------------------------------------------------------\n",
            "üìå Text:\n",
            "\n",
            "('As we look ahead to 2022, which artificial intelligence trends will...\\n'\n",
            " '\\n'\n",
            " '‚Ä¢ Make jobs easier\\n'\n",
            " '‚Ä¢ Increase AI accuracy\\n'\n",
            " '‚Ä¢ Stay ahead of threats\\n'\n",
            " '‚Ä¢ Anticipate conversation flow\\n'\n",
            " '\\n'\n",
            " 'Learn more about emerging AI trends: https://t.co/9zDftUSrPQ')\n",
            "==========\n",
            "üåÄ Sarcasm Detection: not_sarcastic\n",
            "==========\n",
            "\n",
            "üîç Named  (spaCy):\n",
            "  - 2022 (DATE)\n",
            "\n",
            "üîç Named Entities_V2_(HuggingFace):\n",
            "  - AI [MISC] (score=1.00)\n",
            "  - AI [MISC] (score=0.99)\n",
            "==========\n",
            "\n",
            "üß† Concepts:\n",
            "  - jobs_and_careers\n",
            "  - ai_tech\n",
            "\n",
            "üß† Concepts_V2_:\n",
            "==========\n",
            "\n",
            "üîß Aspects:\n",
            "  - we\n",
            "  - which artificial intelligence trends\n",
            "  - jobs\n",
            "  - AI accuracy\n",
            "  - threats\n",
            "  - conversation flow\n",
            "  - emerging AI trends\n",
            "\n",
            "üîß Aspects_V2_:\n",
            "  - intelligence trends\n",
            "  - jobs\n",
            "  - AI accuracy\n",
            "  - threats\n",
            "  - conversation flow\n",
            "  - emerging AI trends\n",
            "====================================================================================================\n",
            "Record #4\n",
            "----------------------------------------------------------------------------------------------------\n",
            "üìå Text:\n",
            "\n",
            "('AI might be only taking over jobs that require repetitive work. Someone that '\n",
            " 'adds value and gives spontaneous effort to society could be promoted. '\n",
            " 'https://t.co/ZhME3xSm3Z')\n",
            "==========\n",
            "üåÄ Sarcasm Detection: not_sarcastic\n",
            "==========\n",
            "\n",
            "üîç Named  (spaCy):\n",
            "\n",
            "üîç Named Entities_V2_(HuggingFace):\n",
            "  - AI [MISC] (score=0.59)\n",
            "==========\n",
            "\n",
            "üß† Concepts:\n",
            "  - jobs_and_careers\n",
            "\n",
            "üß† Concepts_V2_:\n",
            "==========\n",
            "\n",
            "üîß Aspects:\n",
            "  - AI\n",
            "  - jobs\n",
            "  - that\n",
            "  - repetitive work\n",
            "  - Someone\n",
            "  - that\n",
            "  - value\n",
            "  - spontaneous effort\n",
            "  - society\n",
            "\n",
            "üîß Aspects_V2_:\n",
            "  - jobs\n",
            "  - repetitive work\n",
            "  - value\n",
            "  - spontaneous effort\n",
            "  - society\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "  # Process and print results\n",
        "for i, record in enumerate(data[:10]):  # You can remove [:10] to print all\n",
        "    print(\"=\" * 100)\n",
        "    print(f\"Record #{i+1}\")\n",
        "    print(\"-\" * 100)\n",
        "    print(f\"üìå Text:\\n\")\n",
        "    pprint.pprint(f\"{record['text']}\")\n",
        "\n",
        "    print(\"=\"*10)\n",
        "    print(f\"üåÄ Sarcasm Detection: {record.get('sarcasm_label', 'N/A')}\")\n",
        "\n",
        "    print(\"=\"*10)\n",
        "    print(\"\\nüîç Named   Entity Recignition(spaCy):\")\n",
        "    for ent in record.get(\"named_entities\", []):\n",
        "        print(f\"  - {ent['entity']} ({ent['label']})\")\n",
        "\n",
        "    print(\"=\"*10)\n",
        "    print(\"\\nüß† Concepts_v3:\")\n",
        "    for concept in record.get(\"concepts_v3\", []):\n",
        "        print(f\"  - {concept}\")\n",
        "\n",
        "    print(\"=\"*10)\n",
        "    print(\"\\nüîß Aspects:\")\n",
        "    for aspect in record.get(\"aspects\", []):\n",
        "        print(f\"  - {aspect}\")\n",
        "\n",
        "    print(\"\\nüîß Aspects_V2_:\")\n",
        "    for aspect in record.get(\"aspects_v2\", []):\n",
        "        print(f\"  - {aspect}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns_LCE3ilN5V"
      },
      "source": [
        "## old data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiKmY7D1RgKk",
        "outputId": "1c2df9c6-6938-45a0-ec8f-5935ec920ff7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "Record #1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "üìå Text:\n",
            "\n",
            "('This past year, many folks left their jobs to find better, more worthwhile '\n",
            " 'opportunities. Director of Sales Max Johns shares 3 tips for anyone hoping '\n",
            " 'to migrate into the fastest growing field; #machinelearning. #ai '\n",
            " '#datacentric #jobs #gethired https://t.co/ZEZxyPtboZ')\n",
            "üåÄ Sarcasm Detection: not_sarcastic\n",
            "\n",
            "üîç Named Entities:\n",
            "  - This past year (DATE)\n",
            "  - Sales Max Johns (PERSON)\n",
            "  - 3 (CARDINAL)\n",
            "  - # (CARDINAL)\n",
            "  - #ai # (MONEY)\n",
            "\n",
            "üß† Concepts:\n",
            "  - ai_tech\n",
            "  - jobs_and_careers\n",
            "\n",
            "üîß Aspects:\n",
            "  - many folks\n",
            "  - their jobs\n",
            "  - better, more worthwhile opportunities\n",
            "  - Director\n",
            "  - Sales Max Johns\n",
            "  - 3 tips\n",
            "  - anyone\n",
            "  - the fastest growing field\n",
            "  - #machinelearning\n",
            "  - datacentric\n",
            "  - #jobs\n",
            "  - #gethired https://t.co/ZEZxyPtboZ\n",
            "====================================================================================================\n",
            "Record #2\n",
            "----------------------------------------------------------------------------------------------------\n",
            "üìå Text:\n",
            "\n",
            "('Will technologies like #AI continue to change the face of business and #jobs '\n",
            " 'in the new year? Yes, according to this @ToolboxforB2B article. What do you '\n",
            " 'think the biggest #worktrend will be in #2022?     https://t.co/YHMujdIq1n')\n",
            "üåÄ Sarcasm Detection: not_sarcastic\n",
            "\n",
            "üîç Named Entities:\n",
            "  - AI (PERSON)\n",
            "  - # (CARDINAL)\n",
            "  - the new year (DATE)\n",
            "  - @ToolboxforB2B (ORG)\n",
            "  - 2022 (MONEY)\n",
            "\n",
            "üß† Concepts:\n",
            "  - ai_tech\n",
            "  - jobs_and_careers\n",
            "\n",
            "üîß Aspects:\n",
            "  - technologies\n",
            "  - AI\n",
            "  - the face\n",
            "  - business\n",
            "  - #jobs\n",
            "  - the new year\n",
            "  - this @ToolboxforB2B article\n",
            "  - What\n",
            "  - you\n",
            "  - the biggest #worktrend\n",
            "  - https://t.co/YHMujdIq1n\n",
            "====================================================================================================\n",
            "Record #3\n",
            "----------------------------------------------------------------------------------------------------\n",
            "üìå Text:\n",
            "\n",
            "('As we look ahead to 2022, which artificial intelligence trends will...\\n'\n",
            " '\\n'\n",
            " '‚Ä¢ Make jobs easier\\n'\n",
            " '‚Ä¢ Increase AI accuracy\\n'\n",
            " '‚Ä¢ Stay ahead of threats\\n'\n",
            " '‚Ä¢ Anticipate conversation flow\\n'\n",
            " '\\n'\n",
            " 'Learn more about emerging AI trends: https://t.co/9zDftUSrPQ')\n",
            "üåÄ Sarcasm Detection: not_sarcastic\n",
            "\n",
            "üîç Named Entities:\n",
            "  - 2022 (DATE)\n",
            "  - ‚Ä¢ Increase AI (ORG)\n",
            "  - ‚Ä¢ Anticipate (ORG)\n",
            "\n",
            "üß† Concepts:\n",
            "  - ai_tech\n",
            "  - jobs_and_careers\n",
            "\n",
            "üîß Aspects:\n",
            "  - we\n",
            "  - which\n",
            "  - jobs\n",
            "  - threats\n",
            "‚Ä¢ Anticipate conversation flow\n",
            "  - emerging AI trends\n",
            "====================================================================================================\n",
            "Record #4\n",
            "----------------------------------------------------------------------------------------------------\n",
            "üìå Text:\n",
            "\n",
            "('AI might be only taking over jobs that require repetitive work. Someone that '\n",
            " 'adds value and gives spontaneous effort to society could be promoted. '\n",
            " 'https://t.co/ZhME3xSm3Z')\n",
            "üåÄ Sarcasm Detection: not_sarcastic\n",
            "\n",
            "üîç Named Entities:\n",
            "\n",
            "üß† Concepts:\n",
            "  - ai_tech\n",
            "  - jobs_and_careers\n",
            "\n",
            "üîß Aspects:\n",
            "  - AI\n",
            "  - jobs\n",
            "  - that\n",
            "  - repetitive work\n",
            "  - Someone\n",
            "  - that\n",
            "  - value\n",
            "  - spontaneous effort\n",
            "  - society\n",
            "  - https://t.co/ZhME3xSm3Z\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "  # Process and print results\n",
        "for i, record in enumerate(data[:10]):\n",
        "    print(\"=\" * 100)\n",
        "    print(f\"Record #{i+1}\")\n",
        "    print(\"-\" * 100)\n",
        "    print(f\"üìå Text:\\n\")\n",
        "    pprint.pprint(f\"{record['text']}\")\n",
        "\n",
        "    print(\"=\"*10)\n",
        "    print(f\"üåÄ Sarcasm Detection: {record.get('sarcasm_label', 'N/A')}\")\n",
        "\n",
        "    print(\"=\"*10)\n",
        "    print(\"\\nüîç Named Entities:\")\n",
        "    for ent in record.get(\"named_entities\", []):\n",
        "        print(f\"  - {ent['entity']} ({ent['label']})\")\n",
        "\n",
        "    print(\"\\nüîç Named Entities_V2_:\")\n",
        "    for ent in record.get(\"named_entities_v2\", []):\n",
        "        print(f\"  - {ent['entity']} ({ent['label']})\")\n",
        "\n",
        "    print(\"=\"*10)\n",
        "    print(\"\\nüß† Concepts:\")\n",
        "    for concept in record.get(\"concepts\", []):\n",
        "        print(f\"  - {concept}\")\n",
        "\n",
        "    print(\"\\nüß† Concepts_V2_:\")\n",
        "    for concept in record.get(\"concepts_v2\", []):\n",
        "        print(f\"  - {concept}\")\n",
        "\n",
        "    print(\"=\"*10)\n",
        "    print(\"\\nüîß Aspects:\")\n",
        "    for aspect in record.get(\"aspects\", []):\n",
        "        print(f\"  - {aspect}\")\n",
        "\n",
        "    print(\"\\nüîß Aspects_V2_:\")\n",
        "    for aspect in record.get(\"aspects_v2\", []):\n",
        "        print(f\"  - {aspect}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da9ZRAjORf6g"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3WxdPUhQd2O"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
