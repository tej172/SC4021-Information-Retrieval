{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import time"
      ],
      "metadata": {
        "id": "APvYkQgGOr21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers torch"
      ],
      "metadata": {
        "id": "EmGz7GxpO2Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "# 1) SARCASM DETECTION\n",
        "##############################################\n",
        "# Use a pre-trained Hugging Face model specialized in detecting irony/sarcasm.\n",
        "#   Model card: https://huggingface.co/cardiffnlp/twitter-roberta-base-irony\n",
        "# Install: pip install transformers torch\n",
        "# This model is trained on Twitter data\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "SARCASTIC_LABEL = \"sarcastic\"\n",
        "NOT_SARCASTIC_LABEL = \"not_sarcastic\"\n",
        "\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "sarcasm_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "sarcasm_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "def detect_sarcasm(text: str) -> str:\n",
        "    inputs = sarcasm_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        logits = sarcasm_model(**inputs).logits\n",
        "    probs = torch.softmax(logits, dim=1).squeeze()\n",
        "    # Label 0 = non-irony, Label 1 = irony\n",
        "    if probs[1].item() > 0.5:\n",
        "        return SARCASTIC_LABEL\n",
        "    else:\n",
        "        return NOT_SARCASTIC_LABEL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTdao1qFOruq",
        "outputId": "f51f9c3e-5c04-4d18-e9ec-63bbfb3b3323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## part 2"
      ],
      "metadata": {
        "id": "1zdRt2Bj6F4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "id": "cMiFoje_Ornu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "# 2) NAMED ENTITY RECOGNITION (NER)\n",
        "##############################################\n",
        "# We'll use spaCy's English model\n",
        "# Install: pip install spacy\n",
        "# Then: python -m spacy download en_core_web_sm\n",
        "# Then: python -m spacy download en_core_web_trf\n",
        "\n",
        "import spacy\n",
        "\n",
        "# 1) Load an advanced spaCy model (fallback to en_core_web_sm if needed)\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_trf\")\n",
        "    print(\"Using en_core_web_trf for advanced NER.\")\n",
        "except:\n",
        "\n",
        "    # dont fall aback to smallermodel for better results\n",
        "    # nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"Falling back to en_core_web_sm.\")\n",
        "\n",
        "\n",
        "def do_ner(text: str):\n",
        "    \"\"\"\n",
        "    Return a list of recognized named entities (text + label).\n",
        "    Example output: [{\"entity\": \"AI\", \"label\": \"ORG\"}, ...]\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    entities = []\n",
        "    for ent in doc.ents:\n",
        "        entities.append({\"entity\": ent.text, \"label\": ent.label_})\n",
        "\n",
        "    return entities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlyhZMHCOrgL",
        "outputId": "73fcf826-31e2-4c4d-f090-b9ba1b7cfbe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using en_core_web_trf for advanced NER.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## part 3"
      ],
      "metadata": {
        "id": "6aHyAkeT6J-b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvNv8BFOIklu",
        "outputId": "3da1c3a5-97bd-495b-fa4f-cab46546ff64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz\n",
            "Successfully installed rapidfuzz-3.13.0\n"
          ]
        }
      ],
      "source": [
        "! pip install rapidfuzz\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "# 3) CONCEPT EXTRACTION\n",
        "##############################################\n",
        "# Concept extraction can be domain-specific.\n",
        "# that looks for keywords/phrases in the text and groups them under certain \"concepts\".\n",
        "# In real practice, you might use a more sophisticated approach (e.g., ontology mapping).\n",
        "\n",
        "concept_dictionary = {\n",
        "    \"jobs_and_careers\": [\n",
        "        \"job\", \"jobs\", \"career\", \"hire\", \"hiring\", \"position\", \"occupation\", \"employment\",\n",
        "        \"recruitment\", \"headhunting\", \"resum√©\", \"job search\", \"job application\", \"interview\",\n",
        "        \"promotion\", \"layoff\", \"downsizing\", \"resignation\", \"fired\", \"retrenched\", \"workforce\",\n",
        "        \"unemployment\", \"job board\", \"career change\", \"gig economy\", \"contract work\",\n",
        "        \"freelance\", \"upskilling\", \"reskilling\", \"talent shortage\", \"skills gap\",\n",
        "        \"career transition\", \"remote job\", \"flexible work\", \"hybrid work\"\n",
        "    ],\n",
        "\n",
        "    \"ai_tech\": [\n",
        "        \"ai\", \"artificial intelligence\", \"ml\", \"machine learning\", \"deep learning\",\n",
        "        \"neural network\", \"natural language processing\", \"NLP\", \"computer vision\",\n",
        "        \"generative ai\", \"chatgpt\", \"gpt-4\", \"llm\", \"transformer model\", \"reinforcement learning\",\n",
        "        \"automation\", \"autonomous system\", \"robot\", \"ai-powered\", \"ai-driven\",\n",
        "        \"ai algorithm\", \"ai model\", \"ai tool\", \"ai system\", \"ai platform\", \"ai assistant\",\n",
        "        \"predictive modeling\", \"intelligent agent\", \"intelligent automation\", \"digital worker\",\n",
        "        \"AI ethics\", \"AI governance\", \"AI safety\", \"AI alignment\", \"AI regulation\",\n",
        "        \"OpenAI\", \"Hugging Face\", \"LangChain\", \"Chatbot\", \"BERT\", \"GPT\", \"GAN\",\n",
        "        \"AI job disruption\", \"AI replacing jobs\", \"AI impact\", \"AI revolution\"\n",
        "    ],\n",
        "\n",
        "    \"job_market_trends\": [\n",
        "        \"job market\", \"labor market\", \"workforce trends\", \"employment trends\",\n",
        "        \"market shift\", \"skills demand\", \"future of work\", \"career outlook\",\n",
        "        \"talent acquisition\", \"workforce transformation\", \"job automation\",\n",
        "        \"digital transformation\", \"economic uncertainty\", \"AI economy\", \"skills of the future\",\n",
        "        \"AI in hiring\", \"job displacement\", \"employment shift\", \"workplace change\"\n",
        "    ],\n",
        "\n",
        "    \"finance\": [\n",
        "        \"bank\", \"investment\", \"finance\", \"trading\", \"fintech\", \"income\", \"earnings\",\n",
        "        \"financial security\", \"salary\", \"paycheck\", \"minimum wage\", \"wealth gap\",\n",
        "        \"cost of living\", \"job loss compensation\", \"recession\", \"economic downturn\",\n",
        "        \"furlough\", \"financial hardship\", \"severance\", \"retirement fund\"\n",
        "    ],\n",
        "\n",
        "    \"mental_health\": [\n",
        "        \"stress\", \"burnout\", \"anxiety\", \"job stress\", \"mental health\", \"wellbeing\",\n",
        "        \"work-life balance\", \"job insecurity\", \"career anxiety\", \"AI anxiety\"\n",
        "    ],\n",
        "\n",
        "    \"education_training\": [\n",
        "        \"upskilling\", \"reskilling\", \"online course\", \"certification\", \"MOOC\", \"bootcamp\",\n",
        "        \"lifelong learning\", \"career coaching\", \"learning path\", \"digital skills\",\n",
        "        \"AI literacy\", \"tech training\", \"coursework\", \"workforce development\"\n",
        "    ],\n",
        "\n",
        "     \"automation_and_displacement\": [\n",
        "        \"automation\", \"job automation\", \"automated process\", \"workflow automation\",\n",
        "        \"displaced workers\", \"displacement\", \"automated job loss\", \"automated replacement\",\n",
        "        \"robotic process automation\", \"RPA\", \"bots replacing humans\", \"AI takeover\",\n",
        "        \"human redundancy\", \"task automation\"\n",
        "    ],\n",
        "\n",
        "    \"policy_and_governance\": [\n",
        "        \"universal basic income\", \"UBI\", \"AI regulation\", \"labor law\", \"AI tax\",\n",
        "        \"tech policy\", \"future legislation\", \"AI governance\", \"worker protection\",\n",
        "        \"job guarantee\", \"government retraining\", \"economic policy\", \"union response\",\n",
        "        \"digital rights\", \"job security policy\"\n",
        "    ],\n",
        "\n",
        "    \"recruitment_technology\": [\n",
        "        \"AI recruitment\", \"AI hiring\", \"talent filter\", \"algorithmic hiring\",\n",
        "        \"resume screening\", \"candidate ranking\", \"job matching platform\", \"talent intelligence\",\n",
        "        \"hiring automation\", \"virtual interview\", \"digital HR\", \"ATS\", \"HR tech\"\n",
        "    ],\n",
        "\n",
        "    \"remote_and_gig_work\": [\n",
        "        \"gig economy\", \"freelancing\", \"remote job\", \"remote-first\", \"digital nomad\",\n",
        "        \"platform work\", \"side hustle\", \"Uberization\", \"creator economy\", \"independent worker\",\n",
        "        \"contractor\", \"online work\", \"flex work\", \"microtask\", \"Upwork\", \"Fiverr\"\n",
        "    ],\n",
        "\n",
        "    \"public_sentiment_discourse\": [\n",
        "        \"boomer\", \"doomer\", \"tech bro\", \"jobpocalypse\", \"decel\", \"quiet quitting\",\n",
        "        \"great resignation\", \"job hopping\", \"layoff wave\", \"hustle culture\", \"AI hype\",\n",
        "        \"future fear\", \"doomscrolling\", \"LinkedIn post\", \"upskilling frenzy\", \"prompt engineering\"\n",
        "    ],\n",
        "\n",
        "    \"tech_company_trends\": [\n",
        "        \"OpenAI\", \"Google DeepMind\", \"Meta AI\", \"Anthropic\", \"Stability AI\", \"Amazon layoffs\",\n",
        "        \"tech layoffs\", \"hiring freeze\", \"big tech\", \"FAANG\", \"tech exodus\", \"startup layoffs\",\n",
        "        \"VC funding freeze\", \"AI startup\", \"early retirement\", \"LinkedIn hiring trends\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PEyJQ5JT6NA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from spacy.matcher import PhraseMatcher\n",
        "# For fuzzy matching (preferred over fuzzywuzzy)\n",
        "# pip install rapidfuzz\n",
        "from rapidfuzz import fuzz\n",
        "from rapidfuzz import process as fuzz_process\n",
        "\n",
        "# Preprocess PhraseMatcher for multi-word terms\n",
        "phrase_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
        "for cat, keywords in concept_dictionary.items():\n",
        "    patterns = [nlp.make_doc(kw) for kw in keywords if len(kw.split()) > 1]\n",
        "    if patterns:\n",
        "        phrase_matcher.add(cat, patterns)\n",
        "\n",
        "def lemmatized_tokens(doc):\n",
        "    return [token.lemma_.lower() for token in doc if token.is_alpha]\n",
        "\n",
        "def do_concept_extraction_v3(text: str, fuzzy_threshold=88):\n",
        "    doc = nlp(text)\n",
        "    matched_concepts = set()\n",
        "\n",
        "    # Phrase matching (multi-word only)\n",
        "    matches = phrase_matcher(doc)\n",
        "    for match_id, start, end in matches:\n",
        "        matched_concepts.add(nlp.vocab.strings[match_id])\n",
        "\n",
        "    # Token lemmatization\n",
        "    tokens = lemmatized_tokens(doc)\n",
        "\n",
        "    # Fuzzy matching for single-word concepts only\n",
        "    for cat, keywords in concept_dictionary.items():\n",
        "        for kw in keywords:\n",
        "            if len(kw.split()) == 1:\n",
        "                for token in tokens:\n",
        "                    score = fuzz.partial_ratio(token, kw.lower())\n",
        "                    if score >= fuzzy_threshold:\n",
        "                        matched_concepts.add(cat)\n",
        "                        break  # Avoid multiple adds per category\n",
        "\n",
        "    return list(matched_concepts)\n"
      ],
      "metadata": {
        "id": "l2NlsFP56R5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## part4"
      ],
      "metadata": {
        "id": "YINDinYs6YJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "# 4) ASPECT EXTRACTION\n",
        "##############################################"
      ],
      "metadata": {
        "id": "PRidoFvp6WBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Richer Aspect Extraction using/;\n",
        "Instead of returning every noun_chunk, we can use dependency patterns to identify\n",
        "aspects that have ‚Äúdescriptive context‚Äù (e.g., adjectives or prepositional modifiers).\n",
        "\n",
        "We can attempt to collect the head noun plus its modifiers (adjectival, compound)\n",
        "to form a more coherent aspect phrase.\n",
        "'''\n",
        "def do_aspect_extraction_v2(text: str):\n",
        "    \"\"\"\n",
        "    Extract aspects by looking for 'noun phrases' that include\n",
        "    any of the following:\n",
        "      - Adjectival modifiers (amod)\n",
        "      - Compound nouns\n",
        "      - Prepositional modifiers (nmod) linking nouns\n",
        "    This approach can yield more descriptive aspects like: \"powerful AI model\", \"advanced deep learning techniques\"\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    aspects = []\n",
        "\n",
        "    # We'll store aspect phrases by their head noun token\n",
        "    visited_heads = set()\n",
        "\n",
        "    for token in doc:\n",
        "        # If it's a noun that isn't a pronoun or 'dummy' token\n",
        "        if token.pos_ == \"NOUN\" and token.lemma_ not in [\"thing\", \"something\", \"someone\"]:\n",
        "            if token.dep_ in [\"nsubj\", \"dobj\", \"pobj\", \"ROOT\", \"attr\", \"conj\"]:\n",
        "                aspect_span = _expand_aspect(token)\n",
        "                # De-duplicate\n",
        "                if aspect_span not in visited_heads:\n",
        "                    visited_heads.add(aspect_span)\n",
        "                    aspects.append(aspect_span)\n",
        "\n",
        "    return aspects\n",
        "\n",
        "def _expand_aspect(head_token):\n",
        "    \"\"\"\n",
        "    Expand a noun token to include its adjectival or compound modifiers.\n",
        "    We'll gather left-side adjectives, compound nouns, etc.\n",
        "    Example:\n",
        "      Head: \"model\"\n",
        "      Left modifies: \"powerful\", \"AI\"\n",
        "      => \"powerful AI model\"\n",
        "    \"\"\"\n",
        "    # Gather all tokens that are part of this aspect\n",
        "    min_i = head_token.i\n",
        "    max_i = head_token.i\n",
        "    for child in head_token.children:\n",
        "        # If there's a prepositional structure \"model of something\", you can add logic here\n",
        "        pass\n",
        "\n",
        "    # Check left siblings for compounds or amod\n",
        "    for left in reversed(list(head_token.lefts)):\n",
        "        if left.dep_ in [\"amod\", \"compound\", \"nmod\", \"nn\"] or left.pos_ in [\"ADJ\", \"NOUN\"]:\n",
        "            min_i = min(min_i, left.i)\n",
        "        else:\n",
        "            # If we hit a token that isn't describing this noun, break out\n",
        "            break\n",
        "\n",
        "    # Build the text from min_i to max_i\n",
        "    aspect_span = head_token.doc[min_i : max_i + 1].text\n",
        "    return aspect_span"
      ],
      "metadata": {
        "id": "OyVymoY16Z9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## part5"
      ],
      "metadata": {
        "id": "x5Qdxxii6cE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "# PROCESS JSON DATA\n",
        "##############################################\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # when running pls REPLACE WITH YOUR respective FILENAME\n",
        "    # /content/drive/MyDrive/x_data/data.json\n",
        "    # /content/drive/MyDrive/x_data/data_final.json\n",
        "    filename = r\"/content/drive/MyDrive/x_data/evaluationDataset.json\"\n",
        "    output_filename = r\"/content/drive/MyDrive/x_data/evaluationDataset_final.json\"\n",
        "\n",
        "    # Load JSON\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as infile:\n",
        "        data = json.load(infile)\n",
        "\n",
        "    # Process each record\n",
        "    for i, record in enumerate(data):\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            if(i==0):\n",
        "              print(f\"Processing record {i}/{len(data)}\")\n",
        "            else:\n",
        "              print(f\"Processing record {i}/{len(data)} in {time.time()-start_time}sec\")\n",
        "            start_time=time.time()\n",
        "\n",
        "        text = record.get(\"text\", \"\")\n",
        "\n",
        "        # 1) Sarcasm detection\n",
        "        record[\"sarcasm_label\"] = detect_sarcasm(text)  # \"sarcastic\" or \"not_sarcastic\"\n",
        "\n",
        "        # 2) Named Entity Recognition\n",
        "        record[\"named_entities\"] = do_ner(text)\n",
        "        # ner_v2_results = do_ner_v2(text)\n",
        "        # # record[\"concepts_v2\"] = do_concept_extraction_v2(text)\n",
        "        # for item in ner_v2_results:\n",
        "        #   if \"score\" in item:\n",
        "        #     # Convert from np.float32 to standard Python float\n",
        "        #     item[\"score\"] = float(item[\"score\"])\n",
        "        # record[\"named_entities_v2\"] = ner_v2_results\n",
        "\n",
        "        # 3) Concept Extraction\n",
        "        # record[\"concepts\"] = do_concept_extraction(text)\n",
        "        # record[\"concepts_V2\"] = do_concept_extraction_v2(text)\n",
        "        record[\"concepts\"] = do_concept_extraction_v3(text)\n",
        "\n",
        "\n",
        "\n",
        "        # 4) Aspect Extraction\n",
        "        # record[\"aspects\"] = do_aspect_extraction(text)\n",
        "        record[\"aspects\"] = do_aspect_extraction_v2(text)\n",
        "\n",
        "\n",
        "    # Save updated JSON\n",
        "    with open(output_filename, \"w\", encoding=\"utf-8\") as outfile:\n",
        "        json.dump(data, outfile, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(\"All done! Check your output file for updated JSON data.\")\n",
        "    print(f\"Output saved to {output_filename}\")\n",
        "    print(f\"Original data count: {len(data)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKKw9lnl6bfv",
        "outputId": "68da5738-8fe7-480f-a573-60075608517a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing record 0/1000\n",
            "Processing record 100/1000 in 138.61700344085693sec\n",
            "Processing record 200/1000 in 125.30375742912292sec\n",
            "Processing record 300/1000 in 131.96119809150696sec\n",
            "Processing record 400/1000 in 138.91060256958008sec\n",
            "Processing record 500/1000 in 127.3475775718689sec\n",
            "Processing record 600/1000 in 129.9900472164154sec\n",
            "Processing record 700/1000 in 120.318106174469sec\n",
            "Processing record 800/1000 in 165.3344211578369sec\n",
            "Processing record 900/1000 in 156.30491018295288sec\n",
            "All done! Check your output file for updated JSON data.\n",
            "Output saved to /content/drive/MyDrive/x_data/evaluationDataset_final.json\n",
            "Original data count: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "  # Process and print results\n",
        "for i, record in enumerate(data[:10]):  # You can remove [:10] to print all\n",
        "    print(\"=\" * 100)\n",
        "    print(f\"Record #{i+1}\")\n",
        "    print(\"-\" * 100)\n",
        "    print(f\"üìå Text:\\n\")\n",
        "    pprint.pprint(f\"{record['text']}\")\n",
        "\n",
        "    print(\"=\"*10)\n",
        "    print(f\"üåÄ Sarcasm Detection: {record.get('sarcasm_label', 'N/A')}\")\n",
        "\n",
        "    print(\"=\"*10)\n",
        "    print(\"\\nüîç Named  (spaCy):\")\n",
        "    for ent in record.get(\"named_entities\", []):\n",
        "        print(f\"  - {ent['entity']} ({ent['label']})\")\n",
        "\n",
        "    # print(\"\\nüîç Named Entities_V2_(HuggingFace):\")\n",
        "    # for ent in record.get(\"named_entities_v2\", []):\n",
        "    #         # The HF pipeline uses 'entity_group' and 'word' keys:\n",
        "    #         ent_group = ent.get(\"entity_group\", \"\")\n",
        "    #         word = ent.get(\"word\", \"\")\n",
        "    #         score = ent.get(\"score\", 0.0)\n",
        "    #         print(f\"  - {word} [{ent_group}] (score={score:.2f})\")\n",
        "\n",
        "    print(\"=\"*10)\n",
        "    print(\"\\nüß† Concepts:\")\n",
        "    for concept in record.get(\"concepts\", []):\n",
        "        print(f\"  - {concept}\")\n",
        "\n",
        "\n",
        "    print(\"=\"*10)\n",
        "    print(\"\\nüîß Aspects:\")\n",
        "    for aspect in record.get(\"aspects\", []):\n",
        "        print(f\"  - {aspect}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "SrMiHHgq9GmR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c90daafa-2baa-49e1-b7ef-d63969dae306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "Record #1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "üìå Text:\n",
            "\n",
            "('Yes. AI is going to replace bullshit jobs because AI can give bulshit '\n",
            " 'answers just as convincingly as a human. A nutritionist.')\n",
            "==========\n",
            "üåÄ Sarcasm Detection: sarcastic\n",
            "==========\n",
            "\n",
            "üîç Named  (spaCy):\n",
            "==========\n",
            "\n",
            "üß† Concepts:\n",
            "  - public_sentiment_discourse\n",
            "  - education_training\n",
            "  - automation_and_displacement\n",
            "  - mental_health\n",
            "  - recruitment_technology\n",
            "  - finance\n",
            "  - tech_company_trends\n",
            "  - ai_tech\n",
            "  - remote_and_gig_work\n",
            "  - jobs_and_careers\n",
            "==========\n",
            "\n",
            "üîß Aspects:\n",
            "  - bullshit jobs\n",
            "  - bulshit answers\n",
            "  - human\n",
            "  - nutritionist\n",
            "====================================================================================================\n",
            "Record #2\n",
            "----------------------------------------------------------------------------------------------------\n",
            "üìå Text:\n",
            "\n",
            "'What is your job and do you think AI will replace you in the next 10 years?'\n",
            "==========\n",
            "üåÄ Sarcasm Detection: not_sarcastic\n",
            "==========\n",
            "\n",
            "üîç Named  (spaCy):\n",
            "  - the next 10 years (DATE)\n",
            "==========\n",
            "\n",
            "üß† Concepts:\n",
            "  - public_sentiment_discourse\n",
            "  - education_training\n",
            "  - mental_health\n",
            "  - finance\n",
            "  - tech_company_trends\n",
            "  - ai_tech\n",
            "  - remote_and_gig_work\n",
            "  - jobs_and_careers\n",
            "==========\n",
            "\n",
            "üîß Aspects:\n",
            "  - job\n",
            "  - years\n",
            "====================================================================================================\n",
            "Record #3\n",
            "----------------------------------------------------------------------------------------------------\n",
            "üìå Text:\n",
            "\n",
            "('\"AI is transforming the way we work and the skills needed to succeed in the '\n",
            " 'job market. Stay informed about its impact on employment, education, and the '\n",
            " 'future of work. #AI #labormarket #futureofwork #automation '\n",
            " 'https://t.co/gRpj6sMaE3 https://t.co/QKM3J1iNFO')\n",
            "==========\n",
            "üåÄ Sarcasm Detection: not_sarcastic\n",
            "==========\n",
            "\n",
            "üîç Named  (spaCy):\n",
            "==========\n",
            "\n",
            "üß† Concepts:\n",
            "  - public_sentiment_discourse\n",
            "  - education_training\n",
            "  - automation_and_displacement\n",
            "  - mental_health\n",
            "  - finance\n",
            "  - tech_company_trends\n",
            "  - ai_tech\n",
            "  - job_market_trends\n",
            "  - remote_and_gig_work\n",
            "  - jobs_and_careers\n",
            "==========\n",
            "\n",
            "üîß Aspects:\n",
            "  - way\n",
            "  - skills\n",
            "  - job market\n",
            "  - impact\n",
            "  - employment\n",
            "  - education\n",
            "  - future\n",
            "  - work\n",
            "====================================================================================================\n",
            "Record #4\n",
            "----------------------------------------------------------------------------------------------------\n",
            "üìå Text:\n",
            "\n",
            "('I agree with the \"missing context\" part of your comment. I\\'m actually '\n",
            " 'working on a project that attempts to tackle this issue [PR '\n",
            " 'Pilot](https://www.pr-pilot.ai) with an example project called \"What about '\n",
            " 'Jobs\" '\n",
            " '[https://github.com/mlamina/what-about-jobs](https://github.com/mlamina/what-about-jobs) '\n",
            " 'where I sort of try to have AI discuss the issue of Job replacement '\n",
            " 'autonomously')\n",
            "==========\n",
            "üåÄ Sarcasm Detection: not_sarcastic\n",
            "==========\n",
            "\n",
            "üîç Named  (spaCy):\n",
            "  - What about Jobs (WORK_OF_ART)\n",
            "==========\n",
            "\n",
            "üß† Concepts:\n",
            "  - public_sentiment_discourse\n",
            "  - education_training\n",
            "  - automation_and_displacement\n",
            "  - mental_health\n",
            "  - recruitment_technology\n",
            "  - finance\n",
            "  - tech_company_trends\n",
            "  - ai_tech\n",
            "  - remote_and_gig_work\n",
            "  - policy_and_governance\n",
            "  - jobs_and_careers\n",
            "==========\n",
            "\n",
            "üîß Aspects:\n",
            "  - part\n",
            "  - comment\n",
            "  - project\n",
            "  - issue\n",
            "  - example project\n",
            "  - AI\n",
            "  - Job replacement\n",
            "====================================================================================================\n",
            "Record #5\n",
            "----------------------------------------------------------------------------------------------------\n",
            "üìå Text:\n",
            "\n",
            "('Nurse is one of the most AI resistant careers which exist. By the time '\n",
            " 'nurses have been replaced, so will have everything else.')\n",
            "==========\n",
            "üåÄ Sarcasm Detection: sarcastic\n",
            "==========\n",
            "\n",
            "üîç Named  (spaCy):\n",
            "==========\n",
            "\n",
            "üß† Concepts:\n",
            "  - jobs_and_careers\n",
            "  - mental_health\n",
            "  - ai_tech\n",
            "  - remote_and_gig_work\n",
            "  - tech_company_trends\n",
            "==========\n",
            "\n",
            "üîß Aspects:\n",
            "  - Nurse\n",
            "  - resistant careers\n",
            "  - time\n",
            "====================================================================================================\n",
            "Record #6\n",
            "----------------------------------------------------------------------------------------------------\n",
            "üìå Text:\n",
            "\n",
            "('I am sorry to come off rude but who will pay your bills ?Where will you get '\n",
            " 'money ?You have spent 6 years in college, worked another 10 in your field '\n",
            " 'and you have been replaced by an AI.The only things that you can do better '\n",
            " 'than an AI currently is to move boxes in a wearhouse. Anything you might try '\n",
            " 'to learn in the next 5 years and only includes mental activities and not '\n",
            " 'manual labor an AI will do better.So who will give you money for what ?If '\n",
            " 'you think the government or something similar will give you money because '\n",
            " 'now machines are doing the work for you : look at the poverty in Africa or '\n",
            " 'India and you will realize nobody cares about someone else. From 2020 till '\n",
            " 'now 1% of the population grabbed 66% of the newly generated global wealth '\n",
            " 'leaving 34% for the ramaining 99% of the population')\n",
            "==========\n",
            "üåÄ Sarcasm Detection: not_sarcastic\n",
            "==========\n",
            "\n",
            "üîç Named  (spaCy):\n",
            "  - 6 years (DATE)\n",
            "  - 10 (CARDINAL)\n",
            "  - the next 5 years (DATE)\n",
            "  - Africa (LOC)\n",
            "  - India (GPE)\n",
            "  - 2020 (DATE)\n",
            "  - 1% (PERCENT)\n",
            "  - 66% (PERCENT)\n",
            "  - 34% (PERCENT)\n",
            "  - 99% (PERCENT)\n",
            "==========\n",
            "\n",
            "üß† Concepts:\n",
            "  - public_sentiment_discourse\n",
            "  - education_training\n",
            "  - automation_and_displacement\n",
            "  - mental_health\n",
            "  - recruitment_technology\n",
            "  - finance\n",
            "  - tech_company_trends\n",
            "  - ai_tech\n",
            "  - remote_and_gig_work\n",
            "  - policy_and_governance\n",
            "  - jobs_and_careers\n",
            "==========\n",
            "\n",
            "üîß Aspects:\n",
            "  - bills\n",
            "  - money\n",
            "  - years\n",
            "  - college\n",
            "  - field\n",
            "  - AI\n",
            "  - boxes\n",
            "  - wearhouse\n",
            "  - mental activities\n",
            "  - manual labor\n",
            "  - government\n",
            "  - machines\n",
            "  - work\n",
            "  - poverty\n",
            "  - %\n",
            "  - population\n",
            "  - generated global wealth\n",
            "====================================================================================================\n",
            "Record #7\n",
            "----------------------------------------------------------------------------------------------------\n",
            "üìå Text:\n",
            "\n",
            "('AI cannot replace a human worker by doing the exact same work as a human '\n",
            " 'would do, but it can make workers more productive by helping with some '\n",
            " 'tasks, and effectively so, reduce the staffing needs. Basically companies '\n",
            " 'can fire half the employees if AI can help the remaining half to do twice as '\n",
            " 'much work.The other aspect is that management believes AI to replace human '\n",
            " 'workers and uses this as an argument to cut payroll budgets and layoff '\n",
            " 'people, while expecting the remaining lucky survivors to work twice as hard '\n",
            " 'to fulfill the promised efficiency gains.')\n",
            "==========\n",
            "üåÄ Sarcasm Detection: sarcastic\n",
            "==========\n",
            "\n",
            "üîç Named  (spaCy):\n",
            "  - half (CARDINAL)\n",
            "  - half (CARDINAL)\n",
            "==========\n",
            "\n",
            "üß† Concepts:\n",
            "  - public_sentiment_discourse\n",
            "  - education_training\n",
            "  - automation_and_displacement\n",
            "  - mental_health\n",
            "  - recruitment_technology\n",
            "  - finance\n",
            "  - tech_company_trends\n",
            "  - ai_tech\n",
            "  - remote_and_gig_work\n",
            "  - jobs_and_careers\n",
            "==========\n",
            "\n",
            "üîß Aspects:\n",
            "  - human worker\n",
            "  - same work\n",
            "  - human\n",
            "  - workers\n",
            "  - tasks\n",
            "  - staffing needs\n",
            "  - companies\n",
            "  - employees\n",
            "  - remaining half\n",
            "  - much work\n",
            "  - other aspect\n",
            "  - management\n",
            "  - AI\n",
            "  - human workers\n",
            "  - argument\n",
            "  - payroll budgets\n",
            "  - people\n",
            "  - remaining lucky survivors\n",
            "  - promised efficiency gains\n",
            "====================================================================================================\n",
            "Record #8\n",
            "----------------------------------------------------------------------------------------------------\n",
            "üìå Text:\n",
            "\n",
            "('To be honest, digital jobs are a lot easier to be replaced by a digital AI '\n",
            " 'than physical jobs being replaced by AI powered robots, due to the high cost '\n",
            " 'of building robots. So I think plenty of labor jobs like plumbing or '\n",
            " \"construction or gardening (etc) aren't going to be replaced by millions of \"\n",
            " 'robots any time soon.')\n",
            "==========\n",
            "üåÄ Sarcasm Detection: sarcastic\n",
            "==========\n",
            "\n",
            "üîç Named  (spaCy):\n",
            "  - millions (CARDINAL)\n",
            "==========\n",
            "\n",
            "üß† Concepts:\n",
            "  - public_sentiment_discourse\n",
            "  - education_training\n",
            "  - automation_and_displacement\n",
            "  - mental_health\n",
            "  - recruitment_technology\n",
            "  - finance\n",
            "  - tech_company_trends\n",
            "  - ai_tech\n",
            "  - remote_and_gig_work\n",
            "  - policy_and_governance\n",
            "  - jobs_and_careers\n",
            "==========\n",
            "\n",
            "üîß Aspects:\n",
            "  - digital jobs\n",
            "  - digital AI\n",
            "  - physical jobs\n",
            "  - powered robots\n",
            "  - high cost\n",
            "  - robots\n",
            "  - plenty\n",
            "  - labor jobs\n",
            "  - plumbing\n",
            "  - construction\n",
            "  - gardening\n",
            "  - millions\n",
            "====================================================================================================\n",
            "Record #9\n",
            "----------------------------------------------------------------------------------------------------\n",
            "üìå Text:\n",
            "\n",
            "(\"I love AI, but I feel like doomerism is a for of overhype.LOL, I don't buy \"\n",
            " 'that AI is replacing doctors or other medical professionals. The automated '\n",
            " 'systems are good, but all I see that doing is freeing professionals of their '\n",
            " 'traditional duties.')\n",
            "==========\n",
            "üåÄ Sarcasm Detection: not_sarcastic\n",
            "==========\n",
            "\n",
            "üîç Named  (spaCy):\n",
            "==========\n",
            "\n",
            "üß† Concepts:\n",
            "  - public_sentiment_discourse\n",
            "  - education_training\n",
            "  - automation_and_displacement\n",
            "  - mental_health\n",
            "  - recruitment_technology\n",
            "  - finance\n",
            "  - tech_company_trends\n",
            "  - ai_tech\n",
            "  - remote_and_gig_work\n",
            "  - policy_and_governance\n",
            "  - jobs_and_careers\n",
            "==========\n",
            "\n",
            "üîß Aspects:\n",
            "  - AI\n",
            "  - doomerism\n",
            "  - for\n",
            "  - overhype\n",
            "  - doctors\n",
            "  - other medical professionals\n",
            "  - automated systems\n",
            "  - professionals\n",
            "  - traditional duties\n",
            "====================================================================================================\n",
            "Record #10\n",
            "----------------------------------------------------------------------------------------------------\n",
            "üìå Text:\n",
            "\n",
            "('My recent article discusses how the workplace is evolving faster than ever. '\n",
            " 'With AI, automation, demographic shifts, and climate change reshaping '\n",
            " 'industries, the skills we rely on today may not be relevant tomorrow.By '\n",
            " '2030, 39% of skills that drive jobs today will be obsolete. Companies are '\n",
            " \"taking notice85% of employers plan to enhance training programs. But here's \"\n",
            " 'the challenge: 11% of the workforce is at risk of being left behind.The '\n",
            " 'solution? A proactive approach to upskilling, reskilling, and lifelong '\n",
            " 'learning. This is not just about survivalits about thriving in an era where '\n",
            " 'adaptability is the ultimate advantage. Governments, businesses, and '\n",
            " 'individuals must work together to ensure the workforce is ready for the '\n",
            " 'future.How prepared are we for this transformation?Read my full article '\n",
            " 'here:')\n",
            "==========\n",
            "üåÄ Sarcasm Detection: not_sarcastic\n",
            "==========\n",
            "\n",
            "üîç Named  (spaCy):\n",
            "  - today (DATE)\n",
            "  - tomorrow (DATE)\n",
            "  - 2030 (DATE)\n",
            "  - 39% (PERCENT)\n",
            "  - today (DATE)\n",
            "  - 11% (PERCENT)\n",
            "==========\n",
            "\n",
            "üß† Concepts:\n",
            "  - public_sentiment_discourse\n",
            "  - education_training\n",
            "  - automation_and_displacement\n",
            "  - mental_health\n",
            "  - recruitment_technology\n",
            "  - finance\n",
            "  - tech_company_trends\n",
            "  - ai_tech\n",
            "  - remote_and_gig_work\n",
            "  - jobs_and_careers\n",
            "==========\n",
            "\n",
            "üîß Aspects:\n",
            "  - recent article\n",
            "  - workplace\n",
            "  - automation\n",
            "  - demographic shifts\n",
            "  - climate change\n",
            "  - industries\n",
            "  - skills\n",
            "  - %\n",
            "  - jobs\n",
            "  - Companies\n",
            "  - employers\n",
            "  - training programs\n",
            "  - challenge\n",
            "  - workforce\n",
            "  - risk\n",
            "  - solution\n",
            "  - proactive approach\n",
            "  - upskilling\n",
            "  - reskilling\n",
            "  - lifelong learning\n",
            "  - survivalits\n",
            "  - era\n",
            "  - adaptability\n",
            "  - ultimate advantage\n",
            "  - Governments\n",
            "  - businesses\n",
            "  - individuals\n",
            "  - future\n",
            "  - full article\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LVIi97fRH56R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}